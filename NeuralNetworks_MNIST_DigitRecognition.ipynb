{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks-MNIST-DigitRecognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Neural Networks from scratch without using Tensorflow\n",
        "<br><br>\n",
        "The MNIST data set consists of 70,000 images of handwritten digits. It consists of digits from 0 to 9, the requirement is to classify the class to which the image belongs. The images in the MNIST data set are 28X28 pixels, and the input layer shall have 784 neurons (each neuron takes 1 pixel as the input). The output layer shall have 10 neurons, with each giving the probability of the input image belonging to any of the 10 classes. The image is classified into the class that is represented by the neuron with the highest probability. In the diagram below, you can see a few sample images that are available in the dataset.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://images.upgrad.com/80bd8b72-de1d-4ea6-9923-6b9fc3d03ed9-8.png\">\n"
      ],
      "metadata": {
        "id": "ZH2paHvqDgbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fe7q5rQxCS8-"
      },
      "outputs": [],
      "source": [
        "# Import required libraries:\n",
        "# For model file handling\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import scipy\n",
        "\n",
        "# For plotting and image displays\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "KER0N9dVHLPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://cdn.upgrad.com/uploads/production/2f798b87-a95d-4d53-b34b-166f291c022b/mnist.pkl.gz\"\n",
        "# Download the dataset to the local path\n",
        "!wget $data_url\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBSm1AkxGrxL",
        "outputId": "cd3dcdb1-aa2d-4b3f-e060-65cab492fec0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-16 04:35:48--  https://cdn.upgrad.com/uploads/production/2f798b87-a95d-4d53-b34b-166f291c022b/mnist.pkl.gz\n",
            "Resolving cdn.upgrad.com (cdn.upgrad.com)... 54.192.81.121, 54.192.81.35, 54.192.81.105, ...\n",
            "Connecting to cdn.upgrad.com (cdn.upgrad.com)|54.192.81.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/x-gzip]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  16.26M  28.7MB/s    in 0.6s    \n",
            "\n",
            "2022-06-16 04:35:49 (28.7 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n",
            "mnist.pkl.gz  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset\n",
        "* uncompress the dataset file\n",
        "* read the file into python variables"
      ],
      "metadata": {
        "id": "cfcPnCwdHI4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file='mnist.pkl.gz'):\n",
        "  \"\"\"\n",
        "  Function to uncompress .gz file into a .pkl file. Load the .pkl file as \n",
        "  train_data, validation_data and test_data in that order\n",
        "  file: string, .gz filename containing .pkl dataset file\n",
        "  \"\"\"\n",
        "  # Open the file in read-only binary mode using gzip library\n",
        "  file_pointer = gzip.open(file, mode='rb')\n",
        "  # point the file_pointer to the start of the file\n",
        "  file_pointer.seek(0)\n",
        "  # load the three dataset objects\n",
        "  # The data was created using latin1 encoding\n",
        "  train_data, validation_data, test_data = pickle.load(file_pointer,\n",
        "                                                       encoding='latin1')\n",
        "  return train_data, validation_data, test_data"
      ],
      "metadata": {
        "id": "t2j3m2xrGuWg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = load_data()\n",
        "print(f'train data type = {type(train)}, len = {len(train)}')\n",
        "print(f'val data type = {type(val)}, len = {len(val)}')\n",
        "print(f'test data type = {type(test)}, len = {len(test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJkiTlOxJFSR",
        "outputId": "e0b3bda8-d4ed-43d2-f80e-4604ca4097b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data type = <class 'tuple'>, len = 2\n",
            "val data type = <class 'tuple'>, len = 2\n",
            "test data type = <class 'tuple'>, len = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Dimensions of train data,\\n train [0] type = {type(train[0])},\\\n",
        " shape = {train[0].shape}\\n train[1] type ={type(train[1])},\\\n",
        " shape= {train[1].shape}')\n",
        "print('Sample of target values in train data = ', train[1][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9LW9yOXJK-D",
        "outputId": "f1ea6c99-74ab-401e-f4f1-a20f73f7219d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of train data,\n",
            " train [0] type = <class 'numpy.ndarray'>, shape = (50000, 784)\n",
            " train[1] type =<class 'numpy.ndarray'>, shape= (50000,)\n",
            "Sample of target values in train data =  [5 0 4 1 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 50k images each of dimension 28x28 (=784) and 50k targets. We want the image data to be of the form (784, 50000). This can be done by a simple transpose.\n",
        "\n",
        "The targets have to be in a onehot encoded form. A separate function will be written to peerform this operation."
      ],
      "metadata": {
        "id": "D3Yh_88j0xoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoder for the targets\n",
        "def one_hot(labels):\n",
        "  \"\"\"\n",
        "  Function to convert input 1 dim labels into 2 dim one-hot encoded labels\n",
        "  labels: (n,) numpy array\n",
        "  Returns (n, f) numpy array, where f + 1 = number of unique values in labels\n",
        "  \"\"\"\n",
        "  encoder = sklearn.preprocessing.OneHotEncoder()\n",
        "  return encoder.fit_transform(labels.reshape(-1,1)).todense()\n",
        "  \n",
        "# Testing one_hot function\n",
        "one_hot(np.array(range(0,11)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j0HrCCjy9Zs",
        "outputId": "6f3cba73-3040-4000-b9e8-b3a9acd65dcf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making train_data of the shape 784,50_000\n",
        "train_data = train[0].T\n",
        "# Getting the one hot encoded training labels\n",
        "train_label = one_hot(train[1])\n",
        "\n",
        "# Performing the same operations for the test data and validation data\n",
        "test_data = test[0].T\n",
        "test_label=one_hot(test[1])\n",
        "\n",
        "val_data = val[0].T\n",
        "val_label = one_hot(val[1])"
      ],
      "metadata": {
        "id": "CpTu1KSi6LQ6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking a sample image\n",
        "index= 166\n",
        "img = train_data[:,index].reshape((28,28))\n",
        "plt.title('Label is '+ str(train[1][index]))\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "r9s3jFEU3QvU",
        "outputId": "5a04dc08-0cfa-4cd0-f376-9b71308195e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1c07fd0890>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FeedForward Network"
      ],
      "metadata": {
        "id": "fCSNxhM27iEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  Function to compute the sigmoid activation function of the cumulative input z\n",
        "  \"\"\"\n",
        "  h = 1/(1 + np.exp(-z))\n",
        "  # Storing the values of z for backpropagation\n",
        "  sigmoid_memory = z\n",
        "  return h, sigmoid_memory\n",
        "\n",
        "# Testing the sigmoid function\n",
        "sigmoid(np.arange(8).reshape(4,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uLoJgxD7UCr",
        "outputId": "49e3a19b-d9e4-46cb-ab07-165ce0fe118f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.5       , 0.73105858],\n",
              "        [0.88079708, 0.95257413],\n",
              "        [0.98201379, 0.99330715],\n",
              "        [0.99752738, 0.99908895]]), array([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5],\n",
              "        [6, 7]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "  \"\"\"\n",
        "  Function to compute the ReLu activation function\n",
        "  Returns \n",
        "  h: output after applicatioin of the activation function\n",
        "  relu_memory: Same as the input\n",
        "  \"\"\"\n",
        "  h = np.maximum(0,z)\n",
        "  assert(h.shape == z.shape)\n",
        "  relu_memory = z\n",
        "  return h, relu_memory\n",
        "\n",
        "#Testing the relu function\n",
        "relu(np.arange(-4, 4).reshape(-1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXVzElCe8DH5",
        "outputId": "3cc3b0ad-f440-494a-9b2e-5932936f77d5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [2, 3]]), array([[-4, -3],\n",
              "        [-2, -1],\n",
              "        [ 0,  1],\n",
              "        [ 2,  3]]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  \"\"\"\n",
        "  Function to compute the softmax activation function\n",
        "  \"\"\"\n",
        "  h = np.exp(z)/np.sum(np.exp(z), axis=0, keepdims=True)\n",
        "\n",
        "  softmax_memory = z\n",
        "  return h, softmax_memory\n",
        "\n",
        "#Testing the softmax function\n",
        "softmax(np.random.random_sample((3,2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2l6tZID8qjL",
        "outputId": "0ff12cba-1d2e-4d27-8093-e0de5593d5de"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.33680971, 0.38351906],\n",
              "        [0.33393935, 0.38474052],\n",
              "        [0.32925094, 0.23174042]]), array([[0.17534881, 0.52623496],\n",
              "        [0.16679012, 0.52941477],\n",
              "        [0.15265091, 0.02246349]]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Parameters\n",
        "\n",
        "Create a dictionary of weights and biases. Weights shall be initialized randomly and biases shall be initialized at zero"
      ],
      "metadata": {
        "id": "ooaGQW9UJxHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_params(layer_config):\n",
        "  \"\"\"\n",
        "  Function to initialize weights and biases of the Feedforward network\n",
        "  layer_config = list containing the length of layer. the list should include \n",
        "    the length of the input and output layers.\n",
        "    For hidden layers, the value would be the number of neurons. \n",
        "    For input layer, this would be the length of the input dimension. \n",
        "    For output layer, it would be the number of classes at the output\n",
        "  Returns:\n",
        "  dict: Keys would be  W1, W2 ... Wn for weights, b1, b2,... bn for biases\n",
        "      Values would be the corresponding weights and biases\n",
        "  \"\"\"\n",
        "  params = {}\n",
        "  number_of_layers = len(layer_config)\n",
        "  for l in range(1, number_of_layers):\n",
        "    # Create random weights with scaling of 0.1\n",
        "    params[\"W\" + str(l)] = np.random.randn(layer_config[l], \n",
        "                                           layer_config[l-1]) * 0.1\n",
        "    params['b' + str(l)] = np.zeros((layer_config[l],1))\n",
        "\n",
        "    assert(params['W' + str(l)].shape == (layer_config[l],layer_config[l-1]))\n",
        "    assert(params['b' + str(l)].shape == (layer_config[l],1))\n",
        "  return params"
      ],
      "metadata": {
        "id": "cfj4OOrrCKTr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the initialize_param function\n",
        "params = initialize_params([784, 7, 3, 5, 10])\n",
        "params.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7p6fploQqPG",
        "outputId": "d736b27e-929b-4f93-c665-a1d6f6f6793b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4'])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params['W1']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ezFOJ87CLp8",
        "outputId": "21f270e0-642f-4f6a-dd37-1d325e4aa3fe"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09569212, -0.02235143,  0.18484731, ..., -0.0560824 ,\n",
              "        -0.06189288, -0.072475  ],\n",
              "       [-0.04323618,  0.11994046, -0.02544718, ..., -0.12853196,\n",
              "         0.08377511,  0.03346023],\n",
              "       [ 0.19019729, -0.211491  ,  0.03335938, ...,  0.05233743,\n",
              "         0.14310686, -0.0578032 ],\n",
              "       ...,\n",
              "       [-0.10018275, -0.17399205,  0.03897183, ..., -0.05983332,\n",
              "         0.26783265,  0.06479174],\n",
              "       [ 0.04748795,  0.10624506,  0.10151084, ...,  0.20304851,\n",
              "         0.01810277,  0.07137117],\n",
              "       [ 0.08531997, -0.04249314, -0.14588307, ...,  0.00479644,\n",
              "         0.05644999, -0.10865577]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params['b1']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzrvCD4fRDtf",
        "outputId": "616d2f90-e88e-4ec7-e9fa-8a37d88120d8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_forward(h_prev, w, b, activation='relu'):\n",
        "  \"\"\"\n",
        "  Function to calculate cumulative input and apply activation function to it\n",
        "  h_prev: The output from previous layer. For the first layer, this would \n",
        "    be same as the input data\n",
        "  w = weights for the layer\n",
        "  b = bias for the layer. For now the bias is common to all the neurons in a \n",
        "    layer\n",
        "  activation = the activation functioin to apply. This can be 'relu' or \n",
        "    'sigmoid' or 'softmax'\n",
        "  \n",
        "  \"\"\"\n",
        "  # Store the input in a location\n",
        "  linear_memory = (h_prev, w, b)\n",
        "  # calculate the cumulative input to the activation functioin\n",
        "  z = np.matmul(w, h_prev) + b\n",
        "  # Apply the activation function\n",
        "  if activation == 'relu':\n",
        "    h, activation_memory = relu(z)\n",
        "  elif activation == 'sigmoid':\n",
        "    h, activation_memory = sigmoid(z)\n",
        "  else:\n",
        "    h, activation_memory = softmax(z)\n",
        "  return h, linear_memory\n"
      ],
      "metadata": {
        "id": "9jrWzNdATjTm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the layer_forward function\n",
        "# considering that layer l-1 has 2 neurons and layer l has 3 neurons\n",
        "# h_prev = (2x5), w=(3x2) , b=(3,1)\n",
        "# h should be 3x5\n",
        "h_prev = np.array([[2, 3, 4, -5, 6], \n",
        "                   [0, 3, -4, 7, -9]\n",
        "                   ])\n",
        "w = np.array([[1, 3],\n",
        "              [2, 6],\n",
        "              [0, 4]\n",
        "              ])\n",
        "b = np.array([[1], [1], [1]])\n",
        "\n",
        "h = layer_forward(h_prev, w, b )[0]\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwPJHHk5VrMe",
        "outputId": "be984067-b61c-413a-8bb5-77a2849f9f22"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3, 13,  0, 17,  0],\n",
              "       [ 5, 25,  0, 33,  0],\n",
              "       [ 1, 13,  0, 29,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feedforward_network(X, params):\n",
        "  \"\"\"\n",
        "  Function to \n",
        "  \"\"\"\n",
        "  h = X\n",
        "  layers = len(params)//2\n",
        "  memories = []\n",
        "  # Call the layer_forward function for each layer except the output layer\n",
        "  for layer in range(1,layers):\n",
        "    h, memory = layer_forward(h_prev = h,\n",
        "                              w = params[\"W\" + str(layer)],\n",
        "                              b = params[\"b\" + str(layer)],\n",
        "                              activation = 'relu')\n",
        "    memories.append(memory)\n",
        "  # Apply softmax for the final output layer\n",
        "  h, memory = layer_forward(h_prev = h, \n",
        "                            w = params[\"W\" + str(layers)],\n",
        "                            b = params[\"b\" + str(layers)],\n",
        "                            activation='softmax')\n",
        "  memories.append(memory)\n",
        "  return h, memories\n",
        "  "
      ],
      "metadata": {
        "id": "KXeTNeFKXseE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the feedforward_netword function \n",
        "# randomly select a data sample from the training data \n",
        "x_sample = train_data[:,4]\n",
        "\n",
        "h = feedforward_network(x_sample, params)[0]\n",
        "print(h.shape)\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoXMIei4bt4S",
        "outputId": "b6a7b69e-792b-492b-8c74-4d3dba3b14e1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10012259, 0.10003086, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.10020712],\n",
              "       [0.10003362, 0.10000847, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.10005677],\n",
              "       [0.09997068, 0.09999262, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.09995046],\n",
              "       [0.09999903, 0.09999976, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.09999834],\n",
              "       [0.10004827, 0.10001215, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.10008151],\n",
              "       [0.10000778, 0.10000196, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.10001313],\n",
              "       [0.0999308 , 0.09998257, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.09988313],\n",
              "       [0.10001069, 0.10000269, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.10001803],\n",
              "       [0.09998329, 0.09999579, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.09997175],\n",
              "       [0.09989326, 0.09997312, 0.1       , 0.1       , 0.1       ,\n",
              "        0.1       , 0.09981976]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "18PUooROeEyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}